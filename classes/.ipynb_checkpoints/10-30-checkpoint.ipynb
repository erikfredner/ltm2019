{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One quick thing from HW05\n",
    "\n",
    "When you set a variable name as `True` or `False` in a function declaration, that allows your user to decide *how* they want the function to run. It can be used to determine which parts of the function get run in a particular case. Arguments like these are sometimes called `flags`.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weird_print(my_string, reverse_it = False):\n",
    "    if reverse_it is True:\n",
    "        print(my_string[::-1]) # this reverses each character in the string\n",
    "    else:\n",
    "        print(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check it out\n"
     ]
    }
   ],
   "source": [
    "weird_print('check it out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuo ti kcehc\n"
     ]
    }
   ],
   "source": [
    "weird_print('check it out', reverse_it=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the user decides whether to `reverse_it` by passing `True` or `False` to the second argument.\n",
    "\n",
    "We can use the same principle to switch on or off certain parts of our functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocates\n",
    "\n",
    "Last time, Amanda asked about analyzing collocates for specific words. So, I decided to write something to help us do that today.\n",
    "\n",
    "We'll start by gathering a list of texts using our familiar `absolute_paths` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def absolute_paths(directory, txt_only = True):\n",
    "    files = os.listdir(directory)\n",
    "    absolute_paths = []\n",
    "    \n",
    "    for file in files:\n",
    "        path = os.path.join(directory, file)\n",
    "        absolute_paths.append(path)\n",
    "    \n",
    "    if txt_only is True:\n",
    "        txts = []\n",
    "        for x in absolute_paths:\n",
    "            if str('.txt') in str(x):\n",
    "                txts.append(x)\n",
    "        return txts\n",
    "    \n",
    "    else:        \n",
    "        return absolute_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/e/code/literarytextmining/corpora/harry_potter/texts/5 Order of the Phoenix.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/4 Goblet of Fire.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/6 Half-Blood Prince.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/1 Sorcerers Stone.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/3 Prisoner of Azkaban.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/7 Deathly Hallows.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/2 Chamber of Secrets.txt']"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get harry potter paths using our absolute_paths function\n",
    "hp_dir = '/Users/e/code/literarytextmining/corpora/harry_potter/texts'\n",
    "hp_files = absolute_paths(hp_dir)\n",
    "hp_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over our files\n",
    "Now that we have our files, we want to do the following steps.\n",
    "\n",
    "For every file:\n",
    "\n",
    "1. Tokenize it.\n",
    "2. Find all instances of our keyword.\n",
    "3. Count all of the words within a certain number of words on either side of every instance of our keyword.\n",
    "4. Append the resulting dictionary to a list.\n",
    "5. Repeat.\n",
    "\n",
    "Our result is going to be a list of dictionaries that we can drop in to Pandas easily.\n",
    "\n",
    "We'll start with our `tokenize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def tokenize(text, keep_punct = False):\n",
    "    if keep_punct is True:\n",
    "        for punct in string.punctuation:\n",
    "            text = text.replace(punct, ' ' + punct + ' ')\n",
    "    else:\n",
    "        for punct in string.punctuation:\n",
    "            text = text.replace(punct, ' ')\n",
    "    \n",
    "    # this replaces *any* amount of whitespace with a single space using regular expressions\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for x in text.lower().split(' '):\n",
    "        if x.isalpha():\n",
    "            result.append(x)\n",
    "        else:\n",
    "            word = []\n",
    "            for y in x: # for every character\n",
    "                if y.isalpha(): word.append(y)\n",
    "                    \n",
    "            result.append(''.join(word))\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(hp_files[0]).read()\n",
    "tokens = tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'one',\n",
       " 'dudley',\n",
       " 'demented',\n",
       " 'the',\n",
       " 'hottest',\n",
       " 'day',\n",
       " 'of',\n",
       " 'the',\n",
       " 'summer']"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our test word `hagrid`. We want to find every index where `hagrid` appears in the list.\n",
    "\n",
    "We're going to do that using `enumerate` to track the position of our tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "this\n",
      "1\n",
      "that\n",
      "2\n",
      "the other\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(['this', 'that', 'the other']):\n",
    "    print(i)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    if token == 'hagrid':\n",
    "        indexes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27075, 43277, 47354, 53216, 53264]"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says we got 370 instances of `'hagrid'`. Let's `count` to confirm our method worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.count('hagrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use roughly the same ideas we used with our `KWIC` function to get all of our collocates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27075"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cantering',\n",
       " 'softly',\n",
       " 'up',\n",
       " 'and',\n",
       " 'down',\n",
       " 'outside',\n",
       " 'the',\n",
       " 'bedroom',\n",
       " 'door',\n",
       " 'and',\n",
       " 'hagrid',\n",
       " 'the',\n",
       " 'care',\n",
       " 'of',\n",
       " 'magical',\n",
       " 'creatures',\n",
       " 'teacher',\n",
       " 'was',\n",
       " 'saying',\n",
       " 'they']"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[indexes[0]-10:indexes[0]+10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference is that we don't want to count the number of instances of our central word, since it will over-count the number of times that `'hagrid'` is a collocate of itself.\n",
    "\n",
    "But we know that the middlemost element of our list is always going to be our target word, and that we're always going to have an odd number of elements if we take the collocates in equal amounts from each side of the target word, so we can remove that middle value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "hagrid_test = tokens[indexes[0]-10:indexes[0]+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hagrid_test[round(len(hagrid_test)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cantering',\n",
       " 'softly',\n",
       " 'up',\n",
       " 'and',\n",
       " 'down',\n",
       " 'outside',\n",
       " 'the',\n",
       " 'bedroom',\n",
       " 'door',\n",
       " 'and',\n",
       " 'the',\n",
       " 'care',\n",
       " 'of',\n",
       " 'magical',\n",
       " 'creatures',\n",
       " 'teacher',\n",
       " 'was',\n",
       " 'saying',\n",
       " 'they']"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we need to do is extend all of these into a big list of words, and count them up the same way we have been with our full texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "collocates = []\n",
    "\n",
    "for index in indexes:\n",
    "    colls = tokens[index-10:index+10]\n",
    "    del colls[round(len(colls)/2)]\n",
    "    collocates.extend(colls) # we use extend rather than append because we are adding additional elements *from* a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cantering',\n",
       " 'softly',\n",
       " 'up',\n",
       " 'and',\n",
       " 'down',\n",
       " 'outside',\n",
       " 'the',\n",
       " 'bedroom',\n",
       " 'door',\n",
       " 'and',\n",
       " 'the',\n",
       " 'care',\n",
       " 'of',\n",
       " 'magical',\n",
       " 'creatures',\n",
       " 'teacher',\n",
       " 'was',\n",
       " 'saying',\n",
       " 'they',\n",
       " 'of',\n",
       " 'the',\n",
       " 'holidays',\n",
       " 'approached',\n",
       " 'he',\n",
       " 'could',\n",
       " 'not',\n",
       " 'wait',\n",
       " 'to',\n",
       " 'see',\n",
       " 'again',\n",
       " 'to',\n",
       " 'play',\n",
       " 'quidditch',\n",
       " 'even',\n",
       " 'to',\n",
       " 'stroll',\n",
       " 'across',\n",
       " 'the',\n",
       " 'vanished',\n",
       " 'six']"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocates[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just count these using a dictionary like we've done with prior word lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "for coll in collocates:\n",
    "    if coll not in d:\n",
    "        d[coll] = 1\n",
    "    else:\n",
    "        d[coll] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['creatures']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can wrap all of this logic into an abstract function that will work for any book.\n",
    "\n",
    "I use the variable name `horizon` for the number of words that we want to catch on either side of our target term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collocates(filepath, target_word, horizon = 10):\n",
    "    text = open(filepath).read() # get text\n",
    "    tokens = tokenize(text) # get tokens\n",
    "    \n",
    "    indexes = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == target_word:\n",
    "            indexes.append(i) # get indexes\n",
    "    \n",
    "    collocates = []\n",
    "\n",
    "    for index in indexes:\n",
    "        colls = tokens[index-10:index+10]\n",
    "        del colls[round(len(colls)/2)] # don't count target term\n",
    "        collocates.extend(colls) # we use extend rather than append because we are adding additional elements *from* a list\n",
    "        \n",
    "    d = {}\n",
    "    # we want to make sure we get data about where our values are coming from. this tells us the file:\n",
    "    d['filepath'] = os.path.split(filepath)[-1] \n",
    "    d['target_word'] = target_word # this tells us our target\n",
    "    \n",
    "    for coll in collocates:\n",
    "        if coll not in d:\n",
    "            d[coll] = 1 # count up collocates\n",
    "        else:\n",
    "            d[coll] += 1\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/e/code/literarytextmining/corpora/harry_potter/texts/5 Order of the Phoenix.txt'"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_collocates(hp_files[0], 'hagrid') # it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it working for one file, we can loop this function we've written over multiple files, and append our results to a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/e/code/literarytextmining/corpora/harry_potter/texts/5 Order of the Phoenix.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/4 Goblet of Fire.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/6 Half-Blood Prince.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/1 Sorcerers Stone.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/3 Prisoner of Azkaban.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/7 Deathly Hallows.txt',\n",
       " '/Users/e/code/literarytextmining/corpora/harry_potter/texts/2 Chamber of Secrets.txt']"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for file in hp_files:\n",
    "    collocates = get_collocates(file, 'hagrid')\n",
    "    output.append(collocates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hagrid_colls = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>abandon</th>\n",
       "      <th>able</th>\n",
       "      <th>abound</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yowling</th>\n",
       "      <th>zey</th>\n",
       "      <th>zigzagged</th>\n",
       "      <th>zoomed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filepath</th>\n",
       "      <th>target_word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 Sorcerers Stone.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 Chamber of Secrets.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 Prisoner of Azkaban.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 Goblet of Fire.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 Order of the Phoenix.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Half-Blood Prince.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 Deathly Hallows.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 3731 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          a  abandon  able  abound  about  \\\n",
       "filepath                   target_word                                      \n",
       "1 Sorcerers Stone.txt      hagrid        87      NaN   NaN     NaN     20   \n",
       "2 Chamber of Secrets.txt   hagrid        52      NaN   NaN     NaN      9   \n",
       "3 Prisoner of Azkaban.txt  hagrid        75      NaN   1.0     NaN     16   \n",
       "4 Goblet of Fire.txt       hagrid        97      NaN   3.0     NaN     30   \n",
       "5 Order of the Phoenix.txt hagrid       143      1.0   4.0     1.0     17   \n",
       "6 Half-Blood Prince.txt    hagrid        58      NaN   3.0     NaN      3   \n",
       "7 Deathly Hallows.txt      hagrid        56      NaN   1.0     NaN      3   \n",
       "\n",
       "                                        above  abruptly  absence  absent  \\\n",
       "filepath                   target_word                                     \n",
       "1 Sorcerers Stone.txt      hagrid         1.0       NaN      NaN     NaN   \n",
       "2 Chamber of Secrets.txt   hagrid         1.0       1.0      NaN     NaN   \n",
       "3 Prisoner of Azkaban.txt  hagrid         NaN       NaN      NaN     NaN   \n",
       "4 Goblet of Fire.txt       hagrid         1.0       NaN      NaN     NaN   \n",
       "5 Order of the Phoenix.txt hagrid         NaN       2.0      2.0     1.0   \n",
       "6 Half-Blood Prince.txt    hagrid         NaN       NaN      1.0     NaN   \n",
       "7 Deathly Hallows.txt      hagrid         NaN       NaN      NaN     NaN   \n",
       "\n",
       "                                        acceleration  ...  yet  you  young  \\\n",
       "filepath                   target_word                ...                    \n",
       "1 Sorcerers Stone.txt      hagrid                NaN  ...  5.0   27    NaN   \n",
       "2 Chamber of Secrets.txt   hagrid                NaN  ...  NaN   26    1.0   \n",
       "3 Prisoner of Azkaban.txt  hagrid                NaN  ...  2.0   38    NaN   \n",
       "4 Goblet of Fire.txt       hagrid                NaN  ...  6.0   55    NaN   \n",
       "5 Order of the Phoenix.txt hagrid                NaN  ...  2.0   68    NaN   \n",
       "6 Half-Blood Prince.txt    hagrid                NaN  ...  NaN   21    1.0   \n",
       "7 Deathly Hallows.txt      hagrid                1.0  ...  2.0   15    NaN   \n",
       "\n",
       "                                        your  yours  yourself  yowling  zey  \\\n",
       "filepath                   target_word                                        \n",
       "1 Sorcerers Stone.txt      hagrid          5    1.0       2.0      NaN  NaN   \n",
       "2 Chamber of Secrets.txt   hagrid          2    1.0       1.0      NaN  NaN   \n",
       "3 Prisoner of Azkaban.txt  hagrid          5    NaN       1.0      NaN  NaN   \n",
       "4 Goblet of Fire.txt       hagrid          7    NaN       NaN      1.0  NaN   \n",
       "5 Order of the Phoenix.txt hagrid          7    1.0       NaN      NaN  NaN   \n",
       "6 Half-Blood Prince.txt    hagrid          6    NaN       NaN      NaN  NaN   \n",
       "7 Deathly Hallows.txt      hagrid          2    NaN       NaN      NaN  1.0   \n",
       "\n",
       "                                        zigzagged  zoomed  \n",
       "filepath                   target_word                     \n",
       "1 Sorcerers Stone.txt      hagrid             NaN     NaN  \n",
       "2 Chamber of Secrets.txt   hagrid             NaN     NaN  \n",
       "3 Prisoner of Azkaban.txt  hagrid             NaN     NaN  \n",
       "4 Goblet of Fire.txt       hagrid             NaN     NaN  \n",
       "5 Order of the Phoenix.txt hagrid             NaN     NaN  \n",
       "6 Half-Blood Prince.txt    hagrid             NaN     NaN  \n",
       "7 Deathly Hallows.txt      hagrid             2.0     1.0  \n",
       "\n",
       "[7 rows x 3731 columns]"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_colls.set_index(['filepath', 'target_word']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinking the data\n",
    "This is a big data frame with 3,731 columns. We can use the techniques we discussed last time to cut it down to a more manageable size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "hagrid_colls = hagrid_colls.set_index(['filepath', 'target_word']).sort_index() # reassigning the changes above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which ones appear with `'hagrid`` the most in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the            1181.0\n",
       "said            858.0\n",
       "to              689.0\n",
       "and             682.0\n",
       "a               568.0\n",
       "harry           553.0\n",
       "of              486.0\n",
       "he              465.0\n",
       "his             463.0\n",
       "was             428.0\n",
       "in              326.0\n",
       "had             284.0\n",
       "it              276.0\n",
       "at              274.0\n",
       "him             262.0\n",
       "you             250.0\n",
       "that            238.0\n",
       "hagrid          231.0\n",
       "on              220.0\n",
       "as              214.0\n",
       "with            209.0\n",
       "they            197.0\n",
       "hermione        188.0\n",
       "up              179.0\n",
       "i               178.0\n",
       "ron             162.0\n",
       "but             162.0\n",
       "not             154.0\n",
       "all             141.0\n",
       "be              140.0\n",
       "                ...  \n",
       "ripping           1.0\n",
       "furry             1.0\n",
       "france            1.0\n",
       "road              1.0\n",
       "frankly           1.0\n",
       "rubble            1.0\n",
       "frantically       1.0\n",
       "freckles          1.0\n",
       "rub               1.0\n",
       "rows              1.0\n",
       "french            1.0\n",
       "rowle             1.0\n",
       "fret              1.0\n",
       "roused            1.0\n",
       "round             1.0\n",
       "friendly          1.0\n",
       "roughly           1.0\n",
       "friendship        1.0\n",
       "frighten          1.0\n",
       "rough             1.0\n",
       "frightening       1.0\n",
       "rot               1.0\n",
       "rosettes          1.0\n",
       "roots             1.0\n",
       "fruitcake         1.0\n",
       "root              1.0\n",
       "rolls             1.0\n",
       "rocks             1.0\n",
       "roan              1.0\n",
       "zoomed            1.0\n",
       "Length: 3731, dtype: float64"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_colls.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have a lot that are very large, and others that are very small. Let's see how the data is distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3731.00000\n",
       "mean        7.65398\n",
       "std        39.56446\n",
       "min         1.00000\n",
       "25%         1.00000\n",
       "50%         2.00000\n",
       "75%         4.00000\n",
       "max      1181.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_colls.sum().describe() # describe() gives us information about the result above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that 75% of `'hagrid'`'s collocates only appear 4 times across all 7 books.\n",
    "\n",
    "Clearly we should set a pretty high threshold. What's the value at the 90th percentile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "hagrid_sums = hagrid_colls.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3731"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hagrid_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3358"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(hagrid_sums) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_sums.sort_values()[3358] # here, sort_values sorts the result by ascending values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that appear 10 times or more with `'hagrid'` represent the 90th+ percentile of collocates. So let's see those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grawp         10.0\n",
       "shaggy        10.0\n",
       "fact          10.0\n",
       "suddenly      10.0\n",
       "find          10.0\n",
       "given         10.0\n",
       "sure          10.0\n",
       "case          10.0\n",
       "followed      10.0\n",
       "rita          10.0\n",
       "supposed      10.0\n",
       "looks         10.0\n",
       "caught        10.0\n",
       "expelled      10.0\n",
       "someone       10.0\n",
       "lupin         10.0\n",
       "sound         10.0\n",
       "point         10.0\n",
       "stepped       10.0\n",
       "heavily       10.0\n",
       "eaters        10.0\n",
       "heads         10.0\n",
       "every         10.0\n",
       "hard          10.0\n",
       "big           11.0\n",
       "deep          11.0\n",
       "mouth         11.0\n",
       "sounding      11.0\n",
       "sight         11.0\n",
       "small         11.0\n",
       "             ...  \n",
       "be           140.0\n",
       "all          141.0\n",
       "not          154.0\n",
       "but          162.0\n",
       "ron          162.0\n",
       "i            178.0\n",
       "up           179.0\n",
       "hermione     188.0\n",
       "they         197.0\n",
       "with         209.0\n",
       "as           214.0\n",
       "on           220.0\n",
       "hagrid       231.0\n",
       "that         238.0\n",
       "you          250.0\n",
       "him          262.0\n",
       "at           274.0\n",
       "it           276.0\n",
       "had          284.0\n",
       "in           326.0\n",
       "was          428.0\n",
       "his          463.0\n",
       "he           465.0\n",
       "of           486.0\n",
       "harry        553.0\n",
       "a            568.0\n",
       "and          682.0\n",
       "to           689.0\n",
       "said         858.0\n",
       "the         1181.0\n",
       "Length: 373, dtype: float64"
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_sums.sort_values()[3358:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, some of these seem especially related to `'hagrid'`: `grawp`, `shaggy`, `rita`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using stopwords to shrink our collocates\n",
    "But a lot of the highest value words here are not related to `'hagrid'` at all. Those are simply the most frequent words, which collocate a lot with just about any word.\n",
    "\n",
    "There are multiple techniques we could use for getting rid of these. We'll start by using `NLTK`'s standard English stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/e/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these 179 stopwords are generally defined as high frequency / low content words. Because they are so common, they don't tell us much that is meaningful about the results that we're looking at, because they appear frequently *everywhere*.\n",
    "\n",
    "Let's look at a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use these words to shrink our collocates list further? Easy: we can check each of our words against the stopwords list and only retain those that do not appear with the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "hagrid_90th = hagrid_sums.sort_values()[3358:].index # index returns the names of the values here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['grawp', 'shaggy', 'fact', 'suddenly', 'find', 'given', 'sure', 'case',\n",
       "       'followed', 'rita',\n",
       "       ...\n",
       "       'was', 'his', 'he', 'of', 'harry', 'a', 'and', 'to', 'said', 'the'],\n",
       "      dtype='object', length=373)"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_90th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "hagrid_nostops = []\n",
    "\n",
    "for x in hagrid_90th:\n",
    "    if x not in stopwords.words('english'):\n",
    "        hagrid_nostops.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grawp', 'shaggy', 'fact', 'suddenly', 'find']"
      ]
     },
     "execution_count": 901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_nostops[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hagrid_nostops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, let's use this filter on our original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>grawp</th>\n",
       "      <th>shaggy</th>\n",
       "      <th>fact</th>\n",
       "      <th>suddenly</th>\n",
       "      <th>find</th>\n",
       "      <th>given</th>\n",
       "      <th>sure</th>\n",
       "      <th>case</th>\n",
       "      <th>followed</th>\n",
       "      <th>rita</th>\n",
       "      <th>...</th>\n",
       "      <th>dumbledore</th>\n",
       "      <th>like</th>\n",
       "      <th>ter</th>\n",
       "      <th>yeh</th>\n",
       "      <th>back</th>\n",
       "      <th>ron</th>\n",
       "      <th>hermione</th>\n",
       "      <th>hagrid</th>\n",
       "      <th>harry</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filepath</th>\n",
       "      <th>target_word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 Sorcerers Stone.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>79</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 Chamber of Secrets.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>46</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 Prisoner of Azkaban.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 Goblet of Fire.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>110</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 Order of the Phoenix.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>79</td>\n",
       "      <td>59</td>\n",
       "      <td>112</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Half-Blood Prince.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>67</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 Deathly Hallows.txt</th>\n",
       "      <th>hagrid</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>69</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        grawp  shaggy  fact  suddenly  find  \\\n",
       "filepath                   target_word                                        \n",
       "1 Sorcerers Stone.txt      hagrid         NaN     1.0   3.0       4.0   NaN   \n",
       "2 Chamber of Secrets.txt   hagrid         NaN     1.0   NaN       NaN   NaN   \n",
       "3 Prisoner of Azkaban.txt  hagrid         NaN     2.0   NaN       1.0   2.0   \n",
       "4 Goblet of Fire.txt       hagrid         NaN     1.0   1.0       4.0   4.0   \n",
       "5 Order of the Phoenix.txt hagrid         3.0     3.0   6.0       1.0   2.0   \n",
       "6 Half-Blood Prince.txt    hagrid         6.0     2.0   NaN       NaN   2.0   \n",
       "7 Deathly Hallows.txt      hagrid         1.0     NaN   NaN       NaN   NaN   \n",
       "\n",
       "                                        given  sure  case  followed  rita  \\\n",
       "filepath                   target_word                                      \n",
       "1 Sorcerers Stone.txt      hagrid         2.0   1.0   NaN       3.0   NaN   \n",
       "2 Chamber of Secrets.txt   hagrid         NaN   1.0   NaN       NaN   NaN   \n",
       "3 Prisoner of Azkaban.txt  hagrid         1.0   2.0   6.0       4.0   NaN   \n",
       "4 Goblet of Fire.txt       hagrid         3.0   1.0   NaN       2.0  10.0   \n",
       "5 Order of the Phoenix.txt hagrid         NaN   3.0   3.0       NaN   NaN   \n",
       "6 Half-Blood Prince.txt    hagrid         NaN   2.0   NaN       1.0   NaN   \n",
       "7 Deathly Hallows.txt      hagrid         4.0   NaN   1.0       NaN   NaN   \n",
       "\n",
       "                                        ...  dumbledore  like  ter  yeh  back  \\\n",
       "filepath                   target_word  ...                                     \n",
       "1 Sorcerers Stone.txt      hagrid       ...         8.0    14   14   12    17   \n",
       "2 Chamber of Secrets.txt   hagrid       ...        17.0     4    5    3     6   \n",
       "3 Prisoner of Azkaban.txt  hagrid       ...         7.0     6   18   16    24   \n",
       "4 Goblet of Fire.txt       hagrid       ...        29.0    19   23   20    31   \n",
       "5 Order of the Phoenix.txt hagrid       ...        12.0    28   41   37    33   \n",
       "6 Half-Blood Prince.txt    hagrid       ...        11.0    11    7   22     9   \n",
       "7 Deathly Hallows.txt      hagrid       ...         NaN     3    8    8    15   \n",
       "\n",
       "                                        ron  hermione  hagrid  harry  said  \n",
       "filepath                   target_word                                      \n",
       "1 Sorcerers Stone.txt      hagrid        12         1      37     79    84  \n",
       "2 Chamber of Secrets.txt   hagrid        26        10      15     46    65  \n",
       "3 Prisoner of Azkaban.txt  hagrid        29        32      32     70   115  \n",
       "4 Goblet of Fire.txt       hagrid        39        41      57    110   191  \n",
       "5 Order of the Phoenix.txt hagrid        39        79      59    112   281  \n",
       "6 Half-Blood Prince.txt    hagrid        14        17      20     67    89  \n",
       "7 Deathly Hallows.txt      hagrid         3         8      11     69    33  \n",
       "\n",
       "[7 rows x 275 columns]"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_colls[hagrid_nostops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from 3,731 corrleates down to just 275, which is an amount of data we can actually read!\n",
    "\n",
    "Let's see what the top results here are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "hagrid_shrunk = hagrid_colls[hagrid_nostops] # making a new dtm to save the smaller group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "said          858.0\n",
       "harry         553.0\n",
       "hagrid        231.0\n",
       "hermione      188.0\n",
       "ron           162.0\n",
       "back          135.0\n",
       "yeh           118.0\n",
       "ter           116.0\n",
       "like           85.0\n",
       "dumbledore     84.0\n",
       "see            83.0\n",
       "looked         79.0\n",
       "looking        75.0\n",
       "got            75.0\n",
       "one            74.0\n",
       "know           73.0\n",
       "could          70.0\n",
       "asked          67.0\n",
       "go             67.0\n",
       "professor      63.0\n",
       "would          62.0\n",
       "told           59.0\n",
       "door           55.0\n",
       "around         55.0\n",
       "head           50.0\n",
       "get            50.0\n",
       "still          49.0\n",
       "come           47.0\n",
       "hand           46.0\n",
       "right          44.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_shrunk.sum().sort_values(ascending = False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have some of Hagrid's distinctive vocabulary `ter` and `yeh`, as well as his closest relationships with Harry, Hermione, Ron, and Dumbledore. We're getting close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling collocate results relative to the corpus\n",
    "\n",
    "The numbers above represent the *absolute frequencies* of all of the words associated with Hagrid in *Harry Potter*. That might be ok in this context since we're dealing with a corpus that is a unified object. But in other cases, we might want to scale our collocates relative to their total frequency in the corpus.\n",
    "\n",
    "To do that we need to make a data frame containing all of our raw frequencies, then divide each of our collocates by the total number of occurrences of that word in the corpus. The results will look very different!\n",
    "\n",
    "We begin with our `make_dtm` function from last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_dtm(directory, scaled = False):\n",
    "    files = absolute_paths(directory)\n",
    "    \n",
    "    result = [] # empty list where I will append the dictionaries of word counts\n",
    "    \n",
    "    for file in files: # looping over the results\n",
    "        text = open(file).read() # read in text file\n",
    "        tokens = tokenize(text) # make tokens list\n",
    "        d = count_words(tokens) # use count_words to create a dictionary\n",
    "        \n",
    "        if scaled is True:\n",
    "            total_words = sum(list(d.values()))\n",
    "            for key,value in d.items():\n",
    "                d[key] = d[key] / total_words\n",
    "        \n",
    "        # os.path.split() returns the base path and the filename as a pair:\n",
    "        d['filepath'] = os.path.split(file)[-1] # include the _ before filename in case the text contains \"filename\"\n",
    "        result.append(d) # append the unscaled result\n",
    "    \n",
    "    return pd.DataFrame(result).set_index('filepath').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dir = '/Users/e/code/literarytextmining/corpora/harry_potter/texts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dtm = make_dtm(hp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aaaaaaaaargh</th>\n",
       "      <th>aaaaaaaarrrrrgh</th>\n",
       "      <th>aaaaaaand</th>\n",
       "      <th>aaaaaand</th>\n",
       "      <th>aaaaahed</th>\n",
       "      <th>aaaaargh</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aargh</th>\n",
       "      <th>...</th>\n",
       "      <th>zograf</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zonko</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoological</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoomed</th>\n",
       "      <th>zooming</th>\n",
       "      <th>éclairs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filepath</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 Sorcerers Stone.txt</th>\n",
       "      <td>1066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 Chamber of Secrets.txt</th>\n",
       "      <td>1879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 Prisoner of Azkaban.txt</th>\n",
       "      <td>2222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 Goblet of Fire.txt</th>\n",
       "      <td>3680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 Order of the Phoenix.txt</th>\n",
       "      <td>4967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Half-Blood Prince.txt</th>\n",
       "      <td>3323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 Deathly Hallows.txt</th>\n",
       "      <td>3604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 19910 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               a  aaaaaaaaargh  aaaaaaaarrrrrgh  aaaaaaand  \\\n",
       "filepath                                                                     \n",
       "1 Sorcerers Stone.txt       1066           NaN              NaN        NaN   \n",
       "2 Chamber of Secrets.txt    1879           NaN              NaN        NaN   \n",
       "3 Prisoner of Azkaban.txt   2222           NaN              NaN        NaN   \n",
       "4 Goblet of Fire.txt        3680           NaN              1.0        1.0   \n",
       "5 Order of the Phoenix.txt  4967           1.0              NaN        NaN   \n",
       "6 Half-Blood Prince.txt     3323           NaN              NaN        NaN   \n",
       "7 Deathly Hallows.txt       3604           NaN              NaN        NaN   \n",
       "\n",
       "                            aaaaaand  aaaaahed  aaaaargh  aaaah  aaah  aargh  \\\n",
       "filepath                                                                       \n",
       "1 Sorcerers Stone.txt            NaN       NaN       NaN    NaN   NaN    NaN   \n",
       "2 Chamber of Secrets.txt         NaN       NaN       NaN    NaN   NaN    NaN   \n",
       "3 Prisoner of Azkaban.txt        NaN       NaN       NaN    NaN   NaN    NaN   \n",
       "4 Goblet of Fire.txt             1.0       1.0       NaN    NaN   1.0    NaN   \n",
       "5 Order of the Phoenix.txt       NaN       NaN       1.0    1.0   NaN    2.0   \n",
       "6 Half-Blood Prince.txt          NaN       NaN       1.0    1.0   NaN    NaN   \n",
       "7 Deathly Hallows.txt            NaN       NaN       NaN    NaN   NaN    1.0   \n",
       "\n",
       "                            ...  zograf  zombie  zone  zonko  zoo  zoological  \\\n",
       "filepath                    ...                                                 \n",
       "1 Sorcerers Stone.txt       ...     NaN     2.0   NaN    NaN  7.0         NaN   \n",
       "2 Chamber of Secrets.txt    ...     NaN     NaN   NaN    NaN  2.0         NaN   \n",
       "3 Prisoner of Azkaban.txt   ...     NaN     1.0   NaN    1.0  NaN         NaN   \n",
       "4 Goblet of Fire.txt        ...     1.0     NaN   NaN    NaN  NaN         1.0   \n",
       "5 Order of the Phoenix.txt  ...     NaN     NaN   1.0    NaN  NaN         NaN   \n",
       "6 Half-Blood Prince.txt     ...     NaN     NaN   NaN    NaN  NaN         NaN   \n",
       "7 Deathly Hallows.txt       ...     NaN     NaN   NaN    NaN  NaN         NaN   \n",
       "\n",
       "                            zoom  zoomed  zooming  éclairs  \n",
       "filepath                                                    \n",
       "1 Sorcerers Stone.txt        1.0       1      2.0      1.0  \n",
       "2 Chamber of Secrets.txt     NaN       2      NaN      NaN  \n",
       "3 Prisoner of Azkaban.txt    NaN       9      3.0      NaN  \n",
       "4 Goblet of Fire.txt         4.0       9     12.0      NaN  \n",
       "5 Order of the Phoenix.txt   2.0      23      7.0      NaN  \n",
       "6 Half-Blood Prince.txt      NaN       7      2.0      1.0  \n",
       "7 Deathly Hallows.txt        1.0       6      5.0      NaN  \n",
       "\n",
       "[7 rows x 19910 columns]"
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shrink our new DTM down to include just those words we're interested in for `'hagrid'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['grawp', 'shaggy', 'fact', 'suddenly', 'find', 'given', 'sure', 'case',\n",
       "       'followed', 'rita',\n",
       "       ...\n",
       "       'dumbledore', 'like', 'ter', 'yeh', 'back', 'ron', 'hermione', 'hagrid',\n",
       "       'harry', 'said'],\n",
       "      dtype='object', length=275)"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_shrunk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grawp</th>\n",
       "      <th>shaggy</th>\n",
       "      <th>fact</th>\n",
       "      <th>suddenly</th>\n",
       "      <th>find</th>\n",
       "      <th>given</th>\n",
       "      <th>sure</th>\n",
       "      <th>case</th>\n",
       "      <th>followed</th>\n",
       "      <th>rita</th>\n",
       "      <th>...</th>\n",
       "      <th>dumbledore</th>\n",
       "      <th>like</th>\n",
       "      <th>ter</th>\n",
       "      <th>yeh</th>\n",
       "      <th>back</th>\n",
       "      <th>ron</th>\n",
       "      <th>hermione</th>\n",
       "      <th>hagrid</th>\n",
       "      <th>harry</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filepath</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 Sorcerers Stone.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>118</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>142</td>\n",
       "      <td>160</td>\n",
       "      <td>52</td>\n",
       "      <td>182</td>\n",
       "      <td>648</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 Chamber of Secrets.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>22</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>187</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>287</td>\n",
       "      <td>659</td>\n",
       "      <td>286</td>\n",
       "      <td>135</td>\n",
       "      <td>1542</td>\n",
       "      <td>1216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 Prisoner of Azkaban.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27</td>\n",
       "      <td>85</td>\n",
       "      <td>46</td>\n",
       "      <td>19</td>\n",
       "      <td>69</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>139</td>\n",
       "      <td>240</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>368</td>\n",
       "      <td>695</td>\n",
       "      <td>615</td>\n",
       "      <td>201</td>\n",
       "      <td>1814</td>\n",
       "      <td>1511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 Goblet of Fire.txt</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42</td>\n",
       "      <td>115</td>\n",
       "      <td>113</td>\n",
       "      <td>52</td>\n",
       "      <td>96</td>\n",
       "      <td>22</td>\n",
       "      <td>42</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>518</td>\n",
       "      <td>451</td>\n",
       "      <td>55</td>\n",
       "      <td>50</td>\n",
       "      <td>600</td>\n",
       "      <td>975</td>\n",
       "      <td>806</td>\n",
       "      <td>311</td>\n",
       "      <td>2945</td>\n",
       "      <td>2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 Order of the Phoenix.txt</th>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>69</td>\n",
       "      <td>50</td>\n",
       "      <td>133</td>\n",
       "      <td>61</td>\n",
       "      <td>184</td>\n",
       "      <td>52</td>\n",
       "      <td>87</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>536</td>\n",
       "      <td>527</td>\n",
       "      <td>124</td>\n",
       "      <td>75</td>\n",
       "      <td>793</td>\n",
       "      <td>1169</td>\n",
       "      <td>1196</td>\n",
       "      <td>370</td>\n",
       "      <td>3643</td>\n",
       "      <td>3999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 Half-Blood Prince.txt</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>47</td>\n",
       "      <td>62</td>\n",
       "      <td>129</td>\n",
       "      <td>52</td>\n",
       "      <td>130</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>863</td>\n",
       "      <td>352</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>423</td>\n",
       "      <td>780</td>\n",
       "      <td>646</td>\n",
       "      <td>172</td>\n",
       "      <td>2543</td>\n",
       "      <td>2465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 Deathly Hallows.txt</th>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "      <td>136</td>\n",
       "      <td>48</td>\n",
       "      <td>170</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>460</td>\n",
       "      <td>448</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>542</td>\n",
       "      <td>1031</td>\n",
       "      <td>1079</td>\n",
       "      <td>132</td>\n",
       "      <td>2760</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            grawp  shaggy  fact  suddenly  find  given  sure  \\\n",
       "filepath                                                                       \n",
       "1 Sorcerers Stone.txt         NaN     2.0    12        38    24     11    22   \n",
       "2 Chamber of Secrets.txt      NaN     2.0    20        50    54     22    51   \n",
       "3 Prisoner of Azkaban.txt     NaN     3.0    27        85    46     19    69   \n",
       "4 Goblet of Fire.txt          NaN     3.0    42       115   113     52    96   \n",
       "5 Order of the Phoenix.txt   35.0     5.0    69        50   133     61   184   \n",
       "6 Half-Blood Prince.txt       9.0     3.0    47        62   129     52   130   \n",
       "7 Deathly Hallows.txt        10.0     NaN    39        33   136     48   170   \n",
       "\n",
       "                            case  followed  rita  ...  dumbledore  like  ter  \\\n",
       "filepath                                          ...                          \n",
       "1 Sorcerers Stone.txt          4        10   NaN  ...          68   118   50   \n",
       "2 Chamber of Secrets.txt      14        32   NaN  ...         138   187   22   \n",
       "3 Prisoner of Azkaban.txt     38        31   NaN  ...         139   240   49   \n",
       "4 Goblet of Fire.txt          22        42  85.0  ...         518   451   55   \n",
       "5 Order of the Phoenix.txt    52        87  30.0  ...         536   527  124   \n",
       "6 Half-Blood Prince.txt       44        45   1.0  ...         863   352   32   \n",
       "7 Deathly Hallows.txt         37        40  21.0  ...         460   448   11   \n",
       "\n",
       "                            yeh  back   ron  hermione  hagrid  harry  said  \n",
       "filepath                                                                    \n",
       "1 Sorcerers Stone.txt        58   142   160        52     182    648   420  \n",
       "2 Chamber of Secrets.txt     11   287   659       286     135   1542  1216  \n",
       "3 Prisoner of Azkaban.txt    39   368   695       615     201   1814  1511  \n",
       "4 Goblet of Fire.txt         50   600   975       806     311   2945  2673  \n",
       "5 Order of the Phoenix.txt   75   793  1169      1196     370   3643  3999  \n",
       "6 Half-Blood Prince.txt      38   423   780       646     172   2543  2465  \n",
       "7 Deathly Hallows.txt        15   542  1031      1079     132   2760  1978  \n",
       "\n",
       "[7 rows x 275 columns]"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_dtm[hagrid_shrunk.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to sum all of these up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grawp            54.0\n",
       "shaggy           18.0\n",
       "fact            256.0\n",
       "suddenly        433.0\n",
       "find            635.0\n",
       "given           265.0\n",
       "sure            722.0\n",
       "case            211.0\n",
       "followed        287.0\n",
       "rita            137.0\n",
       "supposed        336.0\n",
       "looks           204.0\n",
       "caught          333.0\n",
       "expelled         73.0\n",
       "someone         328.0\n",
       "lupin           734.0\n",
       "sound           312.0\n",
       "point           289.0\n",
       "stepped         153.0\n",
       "heavily         109.0\n",
       "eaters          342.0\n",
       "heads           199.0\n",
       "every           614.0\n",
       "hard            459.0\n",
       "big             175.0\n",
       "deep            236.0\n",
       "mouth           447.0\n",
       "sounding         87.0\n",
       "sight           340.0\n",
       "small           477.0\n",
       "               ...   \n",
       "right          1493.0\n",
       "hand           1160.0\n",
       "come           1103.0\n",
       "still          1697.0\n",
       "get            1519.0\n",
       "head           1308.0\n",
       "around         2225.0\n",
       "door           1297.0\n",
       "told           1053.0\n",
       "would          2253.0\n",
       "professor      1808.0\n",
       "go             1305.0\n",
       "asked          1090.0\n",
       "could          2760.0\n",
       "know           2469.0\n",
       "one            2487.0\n",
       "got            1977.0\n",
       "looking        1759.0\n",
       "looked         2309.0\n",
       "see            1798.0\n",
       "dumbledore     2722.0\n",
       "like           2323.0\n",
       "ter             343.0\n",
       "yeh             286.0\n",
       "back           3155.0\n",
       "ron            5469.0\n",
       "hermione       4680.0\n",
       "hagrid         1503.0\n",
       "harry         15895.0\n",
       "said          14262.0\n",
       "Length: 275, dtype: float64"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_dtm[hagrid_shrunk.columns].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we're going to divide each of Hagrid's collocates by their overall frequency in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grawp         0.185185\n",
       "shaggy        0.555556\n",
       "fact          0.039062\n",
       "suddenly      0.023095\n",
       "find          0.015748\n",
       "given         0.037736\n",
       "sure          0.013850\n",
       "case          0.047393\n",
       "followed      0.034843\n",
       "rita          0.072993\n",
       "supposed      0.029762\n",
       "looks         0.049020\n",
       "caught        0.030030\n",
       "expelled      0.136986\n",
       "someone       0.030488\n",
       "lupin         0.013624\n",
       "sound         0.032051\n",
       "point         0.034602\n",
       "stepped       0.065359\n",
       "heavily       0.091743\n",
       "eaters        0.029240\n",
       "heads         0.050251\n",
       "every         0.016287\n",
       "hard          0.021786\n",
       "big           0.062857\n",
       "deep          0.046610\n",
       "mouth         0.024609\n",
       "sounding      0.126437\n",
       "sight         0.032353\n",
       "small         0.023061\n",
       "                ...   \n",
       "right         0.029471\n",
       "hand          0.039655\n",
       "come          0.042611\n",
       "still         0.028874\n",
       "get           0.032916\n",
       "head          0.038226\n",
       "around        0.024719\n",
       "door          0.042406\n",
       "told          0.056030\n",
       "would         0.027519\n",
       "professor     0.034845\n",
       "go            0.051341\n",
       "asked         0.061468\n",
       "could         0.025362\n",
       "know          0.029567\n",
       "one           0.029755\n",
       "got           0.037936\n",
       "looking       0.042638\n",
       "looked        0.034214\n",
       "see           0.046162\n",
       "dumbledore    0.030860\n",
       "like          0.036591\n",
       "ter           0.338192\n",
       "yeh           0.412587\n",
       "back          0.042789\n",
       "ron           0.029622\n",
       "hermione      0.040171\n",
       "hagrid        0.153693\n",
       "harry         0.034791\n",
       "said          0.060160\n",
       "Length: 275, dtype: float64"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_shrunk.sum() / hp_dtm[hagrid_shrunk.columns].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that result in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "hagrid_colls_scaled = hagrid_shrunk.sum() / hp_dtm[hagrid_shrunk.columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shaggy        0.555556\n",
       "gamekeeper    0.545455\n",
       "gotta         0.500000\n",
       "steak         0.444444\n",
       "yeh           0.412587\n",
       "bin           0.373333\n",
       "fer           0.370787\n",
       "yer           0.340659\n",
       "ter           0.338192\n",
       "skrewts       0.325000\n",
       "fang          0.271739\n",
       "cabin         0.264151\n",
       "maxime        0.244898\n",
       "massive       0.240000\n",
       "creatures     0.217949\n",
       "growled       0.211111\n",
       "beard         0.195402\n",
       "ended         0.194444\n",
       "dragons       0.193548\n",
       "madame        0.192308\n",
       "buckbeak      0.188119\n",
       "grawp         0.185185\n",
       "charlie       0.162500\n",
       "beaming       0.161905\n",
       "hagrid        0.153693\n",
       "visit         0.146789\n",
       "giant         0.143791\n",
       "expelled      0.136986\n",
       "forest        0.130435\n",
       "sounding      0.126437\n",
       "                ...   \n",
       "hard          0.021786\n",
       "place         0.021739\n",
       "trying        0.021676\n",
       "face          0.021615\n",
       "death         0.021248\n",
       "umbridge      0.020873\n",
       "magic         0.020867\n",
       "even          0.020790\n",
       "put           0.020701\n",
       "house         0.020436\n",
       "seemed        0.020390\n",
       "nothing       0.020031\n",
       "black         0.020022\n",
       "hall          0.019576\n",
       "something     0.018949\n",
       "though        0.018847\n",
       "next          0.018786\n",
       "felt          0.018391\n",
       "going         0.018182\n",
       "moment        0.018141\n",
       "every         0.016287\n",
       "find          0.015748\n",
       "mr            0.014178\n",
       "sure          0.013850\n",
       "lupin         0.013624\n",
       "dark          0.013566\n",
       "snape         0.010556\n",
       "weasley       0.010381\n",
       "room          0.008955\n",
       "wand          0.007212\n",
       "Length: 275, dtype: float64"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hagrid_colls_scaled.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it! This gives us words that are *very* closely associated with Hagrid. In some cases, more than half of their occurrences happen within 10 words of Hagrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all this in a function\n",
    "We don't want to repeat those steps for every word. We want results! Let's wrap all of this analysis in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case NLTK isn't cooperating on your system:\n",
    "stopwords = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " \"you'll\",\n",
    " \"you'd\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'ma',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function depends upon a few of our old friends absolute_paths and tokenizer\n",
    "# txt_dir points to a directory where your text files are located, and stored in .txt format\n",
    "\n",
    "def corp_collocates(word, txt_dir, horizon = 10, percentile = 0.9, drop_stopwords = True):\n",
    "    # 1. generate a list of files\n",
    "    filepaths = absolute_paths(txt_dir)\n",
    "    \n",
    "    # 2. make a list of dictionaries containing our data\n",
    "    output = []\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        collocates = get_collocates(filepath, word, horizon)\n",
    "        output.append(collocates)\n",
    "    \n",
    "    # 3. make a dataframe of our results\n",
    "    dtm = pd.DataFrame(output)\n",
    "    dtm = dtm.set_index(['filepath', 'target_word']).sort_index()\n",
    "    \n",
    "    # 4. optionally drop stopwords\n",
    "    keep = []\n",
    "    if drop_stopwords is True:\n",
    "        for x in dtm.columns:\n",
    "            if x not in stopwords:    \n",
    "                keep.append(x)\n",
    "    \n",
    "        dtm = dtm[keep]        \n",
    "        \n",
    "    # 5. sum dtm and cut to percentile\n",
    "    sums = dtm.sum()\n",
    "    pct_index = round(len(sums) * percentile)\n",
    "    top_words = sums.sort_values()[pct_index:].index # index returns the list of words\n",
    "    \n",
    "    # 6. scale results\n",
    "    dtm = dtm[top_words]\n",
    "    raw_values = make_dtm(txt_dir)[top_words]\n",
    "    scaled_results = dtm.sum() / raw_values.sum()\n",
    "    \n",
    "    return scaled_results.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_collocates('your_word', '/Users/erik/Downloads/corp/race/texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "impostors    0.750000\n",
       "rob          0.625000\n",
       "vault        0.305085\n",
       "bank         0.206897\n",
       "goblins      0.142857\n",
       "break        0.069444\n",
       "london       0.058824\n",
       "wizarding    0.048193\n",
       "gringotts    0.048193\n",
       "fer          0.044944\n",
       "goblin       0.042553\n",
       "griphook     0.032000\n",
       "bill         0.023622\n",
       "gold         0.020000\n",
       "white        0.009615\n",
       "work         0.008499\n",
       "bit          0.007678\n",
       "hogwarts     0.006985\n",
       "need         0.006803\n",
       "place        0.006689\n",
       "set          0.006565\n",
       "ever         0.006394\n",
       "inside       0.005865\n",
       "anything     0.005747\n",
       "say          0.005741\n",
       "never        0.005666\n",
       "would        0.004882\n",
       "take         0.004373\n",
       "first        0.004175\n",
       "felt         0.003448\n",
       "something    0.003445\n",
       "moment       0.003401\n",
       "hagrid       0.003327\n",
       "left         0.003286\n",
       "tell         0.003250\n",
       "like         0.003013\n",
       "think        0.002971\n",
       "dark         0.002907\n",
       "toward       0.002885\n",
       "asked        0.002752\n",
       "saw          0.002488\n",
       "know         0.002430\n",
       "time         0.002375\n",
       "see          0.002225\n",
       "still        0.001768\n",
       "ron          0.001646\n",
       "said         0.001472\n",
       "harry        0.001384\n",
       "hermione     0.000855\n",
       "dtype: float64"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example function\n",
    "corp_collocates('gringotts', hp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why does this all matter?\n",
    "\n",
    "Collocates help us see that the words that we associate with specific concepts may or may not appear with those concepts as often as we would expect in our texts.\n",
    "\n",
    "As in the Underwood et al. essay from earlier this week, they did not predict that `grin` and `smile` would separate male- and female-coded behaviors in the mid-20th century. Rather, it emerged from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
