{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More with `nltk`\n",
    "For the homework, you likely used nltk's sentence tokenizing function. However, nltk can do a lot more than just tokenizing sentences.\n",
    "\n",
    "We're going to talk about a few key functions that may be of use for your final projects. Part-of-speech tagging may be especially useful for disambiguiating words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag the parts-of-speech of my words\n",
    "`nltk` makes it easy for us to tag our texts with their parts of speech.\n",
    "\n",
    "This can be useful if you want to only want to look at one version of a homograph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e.g. race (v.) vs. race (n.)\n",
    "Let's take a real example from one of our groups: In the homework assignment on collocation, people found that their collocation results that their results have been made ambiguous by multiple senses of words with the same spelling. \"Race\" as in track-and-field is not as interesting to a group studying ethnicity as \"race,\" as in the socially constructed phenomenon of grouping people by various characteristics.\n",
    "\n",
    "We can use part-of-speech tagging to resolve these ambiguities, and find out which parts-of-speech the collocates of our keywords have too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "We're going to begin by importing `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('book') uncomment this if you are having trouble getting this running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we're going to get our text. For today, we're going to be using James Weldon Johnson's passing narrative, *Autobiography of an Ex-Colored Man* (1912)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "fn = '1912_johnson_ex-colored.txt'\n",
    "fn in os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "johnson = '/Users/e/Downloads/1912_johnson_ex-colored.txt'\n",
    "text = open(johnson).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Autobiography of an Ex-Colored Man\\n\\nJames Weld'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions\n",
    "Now, we need to tokenize our text. We can use the `tokenize` function we wrote.\n",
    "\n",
    "I've added an added feature to deal with the contraction problem we discussed last time, specifically with regard to Obama's State of the Union addresses, and his distinctive words \"don\" and \"t.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\"'em\": '_em',\n",
    " \"'ll\": '_ll',\n",
    " \"'til\": '_til',\n",
    " \"'tis\": '_tis',\n",
    " \"'twas\": '_twas',\n",
    " \"'tween\": '_tween',\n",
    " \"'twere\": '_twere',\n",
    " \"'twill\": '_twill',\n",
    " \"'twixt\": '_twixt',\n",
    " \"'twould\": '_twould',\n",
    " \"'un\": '_un',\n",
    " \"'ve\": '_ve',\n",
    " \"ain't\": 'ain_t',\n",
    " \"amn't\": 'amn_t',\n",
    " \"an'a\": 'an_a',\n",
    " \"an't\": 'an_t',\n",
    " \"anybody'd\": 'anybody_d',\n",
    " \"ar'n't\": 'ar_n_t',\n",
    " \"aren't\": 'aren_t',\n",
    " \"b'hoy\": 'b_hoy',\n",
    " \"br'er\": 'br_er',\n",
    " \"can't\": 'can_t',\n",
    " \"ch'in\": 'ch_in',\n",
    " \"couldn't\": 'couldn_t',\n",
    " \"d'\": 'd_',\n",
    " \"daren't\": 'daren_t',\n",
    " \"dasn't\": 'dasn_t',\n",
    " \"dassn't\": 'dassn_t',\n",
    " \"didn't\": 'didn_t',\n",
    " \"doesn't\": 'doesn_t',\n",
    " \"don't\": 'don_t',\n",
    " \"don'ts\": 'don_ts',\n",
    " \"e'en\": 'e_en',\n",
    " \"e'er\": 'e_er',\n",
    " \"h'm\": 'h_m',\n",
    " \"ha'\": 'ha_',\n",
    " \"ha'nt\": 'ha_nt',\n",
    " \"hadn't\": 'hadn_t',\n",
    " \"hain't\": 'hain_t',\n",
    " \"han't\": 'han_t',\n",
    " \"hasn't\": 'hasn_t',\n",
    " \"haven't\": 'haven_t',\n",
    " \"he'd\": 'he_d',\n",
    " \"he'll\": 'he_ll',\n",
    " \"he's\": 'he_s',\n",
    " \"her'n\": 'her_n',\n",
    " \"his'n\": 'his_n',\n",
    " \"howe'er\": 'howe_er',\n",
    " \"i'd\": 'i_d',\n",
    " \"i'll\": 'i_ll',\n",
    " \"i'm\": 'i_m',\n",
    " \"i've\": 'i_ve',\n",
    " \"in't\": 'in_t',\n",
    " \"isn't\": 'isn_t',\n",
    " \"it'd\": 'it_d',\n",
    " \"it'll\": 'it_ll',\n",
    " \"lor'\": 'lor_',\n",
    " \"ma'am\": 'ma_am',\n",
    " \"mayn't\": 'mayn_t',\n",
    " \"mightn't\": 'mightn_t',\n",
    " \"mustn't\": 'mustn_t',\n",
    " \"n'gana\": 'n_gana',\n",
    " \"ne'er\": 'ne_er',\n",
    " \"needn't\": 'needn_t',\n",
    " \"nobody'd\": 'nobody_d',\n",
    " \"o'\": 'o_',\n",
    " \"o'clock\": 'o_clock',\n",
    " \"o'er\": 'o_er',\n",
    " \"o'ertop\": 'o_ertop',\n",
    " \"oughtn't\": 'oughtn_t',\n",
    " \"our'n\": 'our_n',\n",
    " \"qur'an\": 'qur_an',\n",
    " \"shan't\": 'shan_t',\n",
    " \"she'd\": 'she_d',\n",
    " \"she'll\": 'she_ll',\n",
    " \"she's\": 'she_s',\n",
    " \"shouldn't\": 'shouldn_t',\n",
    " \"somebody'll\": 'somebody_ll',\n",
    " \"someone'll\": 'someone_ll',\n",
    " \"t'\": 't_',\n",
    " \"t'other\": 't_other',\n",
    " \"that'd\": 'that_d',\n",
    " \"that'll\": 'that_ll',\n",
    " \"there'd\": 'there_d',\n",
    " \"there'll\": 'there_ll',\n",
    " \"they'd\": 'they_d',\n",
    " \"they'll\": 'they_ll',\n",
    " \"they're\": 'they_re',\n",
    " \"they've\": 'they_ve',\n",
    " \"this'll\": 'this_ll',\n",
    " \"tho'\": 'tho_',\n",
    " \"thro'\": 'thro_',\n",
    " \"today'll\": 'today_ll',\n",
    " \"wa'\": 'wa_',\n",
    " \"wasn't\": 'wasn_t',\n",
    " \"we'd\": 'we_d',\n",
    " \"we'll\": 'we_ll',\n",
    " \"we're\": 'we_re',\n",
    " \"we've\": 'we_ve',\n",
    " \"weren't\": 'weren_t',\n",
    " \"what'd\": 'what_d',\n",
    " \"what'll\": 'what_ll',\n",
    " \"what're\": 'what_re',\n",
    " \"what've\": 'what_ve',\n",
    " \"what's\": 'what_s',\n",
    " \"whate'er\": 'whate_er',\n",
    " \"whatsoe'er\": 'whatsoe_er',\n",
    " \"when'd\": 'when_d',\n",
    " \"when'll\": 'when_ll',\n",
    " \"when're\": 'when_re',\n",
    " \"when's\": 'when_s',\n",
    " \"whene'er\": 'whene_er',\n",
    " \"whensoe'er\": 'whensoe_er',\n",
    " \"where'd\": 'where_d',\n",
    " \"where'er\": 'where_er',\n",
    " \"where'll\": 'where_ll',\n",
    " \"where're\": 'where_re',\n",
    " \"where's\": 'where_s',\n",
    " \"where've\": 'where_ve',\n",
    " \"wheresoe'er\": 'wheresoe_er',\n",
    " \"who'd\": 'who_d',\n",
    " \"who'll\": 'who_ll',\n",
    " \"who're\": 'who_re',\n",
    " \"who's\": 'who_s',\n",
    " \"who've\": 'who_ve',\n",
    " \"why'll\": 'why_ll',\n",
    " \"why're\": 'why_re',\n",
    " \"why's\": 'why_s',\n",
    " \"won't\": 'won_t',\n",
    " \"wouldn't\": 'wouldn_t',\n",
    " \"you'd\": 'you_d',\n",
    " \"you'll\": 'you_ll',\n",
    " \"you're\": 'you_re',\n",
    " \"you've\": 'you_ve',\n",
    " \"your'n\": 'your_n'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a new function to collapse contractions using the dictionary above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_contractions(text, contractions = contractions):\n",
    "    text = text.lower()\n",
    "    \n",
    "    for key,value in contractions.items():\n",
    "        if key in text:\n",
    "            text = text.replace(key,value) # the key is the form with the apostrophe; the value does not have it\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ain_t' in collapse_contractions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def tokenize(text, keep_punct = False):\n",
    "    # NEW\n",
    "    puncts = list(string.punctuation)\n",
    "    puncts.remove('_') # keep _ characters\n",
    "\n",
    "    if keep_punct is True:\n",
    "        for punct in puncts:\n",
    "            text = text.replace(punct, ' ' + punct + ' ')\n",
    "    else:\n",
    "        for punct in puncts:\n",
    "            text = text.replace(punct, ' ')\n",
    "    \n",
    "    # this replaces *any* amount of whitespace with a single space using regular expressions\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for x in text.lower().split(' '):\n",
    "        if x.isalpha():\n",
    "            result.append(x)\n",
    "        else:\n",
    "            word = []\n",
    "            for y in x: # for every character\n",
    "                if y.isalpha() or y == '_': # retain our underscores\n",
    "                    word.append(y)\n",
    "            if len(word) > 0:\n",
    "                result.append(''.join(word))\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"Wouldn't it be great if we could've caught all of those shortened words like don't?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wouldn_t',\n",
       " 'it',\n",
       " 'be',\n",
       " 'great',\n",
       " 'if',\n",
       " 'we',\n",
       " 'could_ve',\n",
       " 'caught',\n",
       " 'all',\n",
       " 'of',\n",
       " 'those',\n",
       " 'shortened',\n",
       " 'words',\n",
       " 'like',\n",
       " 'don_t']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(collapse_contractions(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Johnson's novel\n",
    "We're going to split *Autobiography* up by sentences to make sure that we keep our text grouped semantically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_sents = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I never saw her read one of them.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_sents[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's preserve contractions\n",
    "text = collapse_contractions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i never saw her read one of them.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to tokenize every sentence in our list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = []\n",
    "\n",
    "for sent in sents:\n",
    "    tokenized_sents.append(tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'never', 'saw', 'her', 'read', 'one', 'of', 'them']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging sentences\n",
    "\n",
    "Let's try running `nltk`'s `pos_tag` function on our tokens in each sentence. The 'universal' tagset gives an easy-to-read description of the parts of speech we're looking at.\n",
    "\n",
    "(It's also a perfect commentary on the universalizing impulses of computation that Risam critiques in the reading for today.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NOUN'),\n",
       " ('never', 'ADV'),\n",
       " ('saw', 'VERB'),\n",
       " ('her', 'PRON'),\n",
       " ('read', 'VERB'),\n",
       " ('one', 'NUM'),\n",
       " ('of', 'ADP'),\n",
       " ('them', 'PRON')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pos is shorthand for 'parts of speech'\n",
    "nltk.pos_tag(tokenized_sents[49], tagset = 'universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the table describing the meaning and examples of nltk's 'universal' tagset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ﻿Tag  | Meaning             | English Examples                       |\n",
    "|------|---------------------|----------------------------------------|\n",
    "| ADJ  | adjective           | new, good, high, special, big, local   |\n",
    "| ADP  | adposition          | on, of, at, with, by, into, under      |\n",
    "| ADV  | adverb              | really, already, still, early, now     |\n",
    "| CONJ | conjunction         | and, or, but, if, while, although      |\n",
    "| DET  | determiner, article | the, a, some, most, every, no, which   |\n",
    "| NOUN | noun                | year, home, costs, time, Africa        |\n",
    "| NUM  | numeral             | twenty-four, fourth, 1991, 14:24       |\n",
    "| PRT  | particle            | at, on, out, over per, that, up, with  |\n",
    "| PRON | pronoun             | he, their, her, its, my, I, us         |\n",
    "| VERB | verb                | is, say, told, given, playing, would   |\n",
    "| .    | punctuation marks   | . , ; !                                |\n",
    "| X    | other               | ersatz, esprit, dunno, gr8, univeristy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run `pos_tag` without the \"universal\" tagset, you will get results keyed to the Penn-Treebank tags. These contain more information, but also a greater number of possible categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('never', 'RB'),\n",
       " ('saw', 'VBD'),\n",
       " ('her', 'PRP'),\n",
       " ('read', 'VB'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('them', 'PRP')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized_sents[49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the Penn-Treebank tags:\n",
    "\n",
    "| ﻿Tag  | Description                               | Example                    |\n",
    "|------|-------------------------------------------|----------------------------|\n",
    "| CC   | conjunction, coordinating                 | and, or, but               |\n",
    "| CD   | cardinal number                           | five, three, 13%           |\n",
    "| DT   | determiner                                | the, a, these              |\n",
    "| EX   | existential there                         | there were six boys        |\n",
    "| FW   | foreign word                              | mais                       |\n",
    "| IN   | conjunction, subordinating or preposition | of, on, before, unless     |\n",
    "| JJ   | adjective                                 | nice, easy                 |\n",
    "| JJR  | adjective, comparative                    | nicer, easier              |\n",
    "| JJS  | adjective, superlative                    | nicest, easiest            |\n",
    "| LS   | list item marker                          |                            |\n",
    "| MD   | verb, modal auxillary                     | may, should                |\n",
    "| NN   | noun, singular or mass                    | tiger, chair, laughter     |\n",
    "| NNS  | noun, plural                              | tigers, chairs, insects    |\n",
    "| NNP  | noun, proper singular                     | Germany, God, Alice        |\n",
    "| NNPS | noun, proper plural                       | we met two Christmases ago |\n",
    "| PDT  | predeterminer                             | both his children          |\n",
    "| POS  | possessive ending                         | 's                         |\n",
    "| PRP  | pronoun, personal                         | me, you, it                |\n",
    "| PRP£ | pronoun, possessive                       | my, your, our              |\n",
    "| RB   | adverb                                    | extremely, loudly, hard    |\n",
    "| RBR  | adverb, comparative                       | better                     |\n",
    "| RBS  | adverb, superlative                       | best                       |\n",
    "| RP   | adverb, particle                          | about, off, up             |\n",
    "| SYM  | symbol                                    | %                          |\n",
    "| TO   | infinitival to                            | what to do?                |\n",
    "| UH   | interjection                              | oh, oops, gosh             |\n",
    "| VB   | verb, base form                           | think                      |\n",
    "| VBZ  | verb, 3rd person singular present         | she thinks                 |\n",
    "| VBP  | verb, non-3rd person singular present     | I think                    |\n",
    "| VBD  | verb, past tense                          | they thought               |\n",
    "| VBN  | verb, past participle                     | a sunken ship              |\n",
    "| VBG  | verb, gerund or present participle        | thinking is fun            |\n",
    "| WDT  | wh-determiner                             | which, whatever, whichever |\n",
    "| WP   | wh-pronoun, personal                      | what, who, whom            |\n",
    "| WP£  | wh-pronoun, possessive                    | whose, whosever            |\n",
    "| WRB  | wh-adverb                                 | where, when                |\n",
    "| .    | punctuation mark, sentence closer         | .;?*                       |\n",
    "| ,    | punctuation mark, comma                   | ,                          |\n",
    "| :    | punctuation mark, colon                   | :                          |\n",
    "| (    | contextual separator, left paren          | (                          |\n",
    "| )    | contextual separator, right paren         | )                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penn-Treebank gives you information about tense (past, present, etc.) which may be useful depending on your purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What sort of data is this result?\n",
    "`nltk`'s part-of-speech tagger returns a **list of tuples**. We have encountered tuples before: they are immutable objects contained in `()` and separated by `,`. They can contain any sort of data. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'b')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('a','b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'a')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They use the same `[]` indexing that we have gotten so accustomed to with lists and data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1, 'a')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these items in the list contains the word from the text in the first position, and its part of speech in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nltk.pos_tag(tokenized_sents[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('never', 'RB'),\n",
       " ('saw', 'VBD'),\n",
       " ('her', 'PRP'),\n",
       " ('read', 'VB'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('them', 'PRP')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i', 'NN')"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That also means that we can do the following commands to get either **piece** of the tuple, which will be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i', 'NN')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're doing above is getting the tuple via `result[0]` and then asking for position `[0]` or `[1]` from that tuple.\n",
    "\n",
    "FYI, if you call out of range, you do get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9e55b2c7751e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "result[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we use this?\n",
    "Let's stick with our example: Can we find all of the sentences in *Autobiography of an Ex-Colored Man* where \"race\" is a noun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I never saw her read one of them.'"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_sents[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'never', 'saw', 'her', 'read', 'one', 'of', 'them']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to `pos_tag` all of our sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = []\n",
    "\n",
    "for sent in tokenized_sents:\n",
    "    pos = nltk.pos_tag(sent, tagset='universal')\n",
    "    tagged_sents.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NOUN'),\n",
       " ('never', 'ADV'),\n",
       " ('saw', 'VERB'),\n",
       " ('her', 'PRON'),\n",
       " ('read', 'VERB'),\n",
       " ('one', 'NUM'),\n",
       " ('of', 'ADP'),\n",
       " ('them', 'PRON')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the autobiography of an ex-colored man\n",
      "\n",
      "james weldon johnson\n",
      "\n",
      "boston: sherman, french & company, 1912\n",
      "copyright, 1912\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "preface\n",
      "\n",
      "this vivid and startlingly new picture of conditions brought about by the race question in the united states makes no special plea for the negro, but shows in a dispassionate, though sympathetic, manner conditions as they actually exist between the whites and blacks to-day.\n",
      "--------------------------------------------------------------------------------\n",
      "this is because writers, in nearly every instance, have treated the colored american as a whole; each has taken some one group of the race to prove his case.\n",
      "--------------------------------------------------------------------------------\n",
      "not before has a composite and proportionate presentation of the entire race, embracing all of its various groups and elements, showing their relations with each other and to the whites, been made.\n",
      "--------------------------------------------------------------------------------\n",
      "in these pages it is as though a veil had been drawn aside: the reader is given a view of the inner life of the negro in america, is initiated into the \"free-masonry,\" as it were, of the race.\n",
      "--------------------------------------------------------------------------------\n",
      "these pages also reveal the unsuspected fact that prejudice against the negro is exerting a pressure, which, in new york and other large cities where the opportunity is open, is actually and constantly forcing an unascertainable number of fair-complexioned colored people over into the white race.\n",
      "--------------------------------------------------------------------------------\n",
      "in this book the reader is given a glimpse behind the scenes of this race-drama which is being here enacted,--he is taken upon an elevation where he can catch a bird_s-eye view of the conflict which is being waged.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,sent in enumerate(tagged_sents[:10]): # using enumerate to count where we are in our sentences, just doing 10\n",
    "    for tup in sent:\n",
    "        if tup[0] == 'race' and tup[1] == 'NOUN':\n",
    "            print(sents[i]) # using the original sents variable to make results easier to read\n",
    "            print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there cases in which \"race\" is not used as a noun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "n_instances = 0\n",
    "\n",
    "for i,sent in enumerate(tagged_sents):\n",
    "    for tup in sent:\n",
    "        if tup[0] == 'race' and tup[1] != 'NOUN':\n",
    "            n_instances += 1 # checking to see how many times it occurs\n",
    "            print(sents[i])\n",
    "            print('-'*80)\n",
    "    \n",
    "print(n_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about other homographs such as \"like\"? It can be a verb, a preposition, a noun...\n",
    "\n",
    "For this case, I'm going to create a list where I will store the parts-of-speech associated with our target word, `like`, and count the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes = []\n",
    "\n",
    "for sent in tagged_sents:\n",
    "    for tup in sent:\n",
    "        if tup[0] == 'like':\n",
    "            likes.append(tup[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for x in likes:\n",
    "    if x not in d:\n",
    "        d[x] = 1\n",
    "    else:\n",
    "        d[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADP': 52, 'VERB': 1, 'ADJ': 4}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ADP': 52, 'VERB': 1, 'ADJ': 4})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(likes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about finding sentences with specific grammatical features?\n",
    "Let's say we wanted to see every sentence in this novel with a past-tense verb, comparative adverbs like \"better,\" and maybe personal wh-pronouns like \"who.\"\n",
    "\n",
    "This could be a way to see how the protagonist compares his new situation as an \"ex-colored man\" to his expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first need to re-tag with the Penn-Treebank set\n",
    "tagged_sents = []\n",
    "\n",
    "for sent in tokenized_sents:\n",
    "    pos = nltk.pos_tag(sent)\n",
    "    tagged_sents.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('never', 'RB'),\n",
       " ('saw', 'VBD'),\n",
       " ('her', 'PRP'),\n",
       " ('read', 'VB'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('them', 'PRP')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags for the elements we want are `'VBD'` and `'RBR'` and `WP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i knew later that these letters contained money and, what was to her, more than money.\n",
      "--------------------------------------------------------------------------------\n",
      "this was the first word missed, and it seemed to me that some of the scholars were about to lose their senses; some were dancing up and down on one foot with a hand above their heads, the fingers working furiously, and joy beaming all over their faces; others stood still, their hands raised not so high, their fingers working less rapidly, and their faces expressing not quite so much happiness; there were still others who did not move nor raise their hands, but stood with great wrinkles on their foreheads, looking very thoughtful.\n",
      "--------------------------------------------------------------------------------\n",
      "i could see that her skin was almost brown, that her hair was not so soft as mine, and that she did differ in some way from the other ladies who came to the house; yet, even so, i could see that she was very beautiful, more beautiful than any of them.\n",
      "--------------------------------------------------------------------------------\n",
      "more than once i had been on the point of recalling to her the promise she had made me, but i instinctively felt that she was happier for not telling me and that i was happier for not being told; yet i had not the slightest idea what the real truth was.\n",
      "--------------------------------------------------------------------------------\n",
      "a great deal has been said about the heart of a girl when she stands \"where the brook and river meet,\" but what she feels is negative; more interesting is the heart of a boy when just at the budding dawn of manhood he stands looking wide-eyed into the long vistas opening before him; when he first becomes conscious of the awakening and quickening of strange desires and unknown powers; when what he sees and feels is still shadowy and mystical enough to be intangible, and, so, more beautiful; when his imagination is unsullied, and his faith new and whole--then it is that love wears a halo--the man who has not loved before he was fourteen has missed a fore-taste of elysium.\n",
      "--------------------------------------------------------------------------------\n",
      "the older i grew the more thought i gave to the question of my and my mother's position, and what was our exact relation to the world in general.\n",
      "--------------------------------------------------------------------------------\n",
      "the president gave me a cordial welcome; it was more than cordial; he talked to me, not as the official head of a college, but as though he were adopting me into what was his large family, to personally look after my general welfare as well as my education.\n",
      "--------------------------------------------------------------------------------\n",
      "as a class the workmen were careless and improvident; some very rapid makers would not work more than three or four days out of the week, and there were others who never showed up at the factory on mondays.\n",
      "--------------------------------------------------------------------------------\n",
      "for example, the proudest and fairest lady in the south could with propriety--and it is what she would most likely do--go to the cabin of aunt mary her cook, if aunt mary were sick, and minister to comfort with her own hands; but if mary's daughter, eliza, a girl who used to run around my lady's kitchen, but who has received an education and married a prosperous young colored man, were at death's door, my lady would no more think of crossing the threshold of eliza's cottage than she would of going into a bar-room for a drink.\n",
      "--------------------------------------------------------------------------------\n",
      "however that may be, there is to my mind no more pathetic side of this many sided question than the isolated position into which are forced the very colored people who most need and who could best appreciate sympathetic coöperation; and their position grows tragic when the effort is made to couple them, whether or no, with the negroes of the first class i mentioned.\n",
      "--------------------------------------------------------------------------------\n",
      "jacksonville, when i was there, was a small town, and the number of educated and well-to-do colored people was few; so this society phase of life did not equal what i have since seen in boston, washington, richmond, and nashville; and it is upon what i have more recently seen in these cities that i have made the observations just above.\n",
      "--------------------------------------------------------------------------------\n",
      "those who were on terms of approach immediately showed their privilege over others less fortunate by gathering around their divinity.\n",
      "--------------------------------------------------------------------------------\n",
      "but had i not escaped it, i would have been no more unfortunate than are many young colored men who come to new york.\n",
      "--------------------------------------------------------------------------------\n",
      "i thought, here i am a man, no longer a boy, and what am i doing but wasting my time and abusing my talent.\n",
      "--------------------------------------------------------------------------------\n",
      "d_o you know the only original contribution to civilization we can claim is what we have done in steam and electricity and in making implements of war more deadly; and there we worked largely on principles which we did not discover.\n",
      "--------------------------------------------------------------------------------\n",
      "he convinced me that, after all, eloquence consists more in the manner of saying than in what is said.\n",
      "--------------------------------------------------------------------------------\n",
      "my few days of pleasure made appalling inroads upon what cash i had, and caused me to see that it required a good deal of money to live in new york as i wished to live, and that i should have to find, very soon, some more or less profitable employment.\n",
      "--------------------------------------------------------------------------------\n",
      "we were standing leaning on the rail in front of a group of figures, more interested in what we had to say to each other than in the group, when my attention became fixed upon a man who stood at my side studying his catalogue.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,sent in enumerate(tagged_sents):\n",
    "    pos = [] # initializing a list to store parts of speech\n",
    "    for tup in sent:\n",
    "        pos.append(tup[1])\n",
    "    if 'VBD' in pos and 'RBR' in pos and 'WP' in pos:\n",
    "        print(sents[i]) # again, using original sents for ease of reading\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these results are exactly what we were looking for! Comparisons between the protagonist's situation, and his expectations about the world.\n",
    "\n",
    "Of course you could do the above analysis using any combination of parts-of-speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting parts of speech\n",
    "\n",
    "Finally, what if we simply wanted to see the most frequent examples of certain parts of speech?\n",
    "\n",
    "Let's look at the comparative adjectives Johnson uses (\"better,\" etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "d = Counter() # initializing a counter object to make collecting instances easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,sent in enumerate(tagged_sents):\n",
    "    for tup in sent:\n",
    "        if tup[1] == 'JJ':\n",
    "            d[tup[0]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can put this data into a familiar Pandas object to sort it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i          203\n",
       "white      101\n",
       "great       97\n",
       "new         83\n",
       "other       83\n",
       "colored     75\n",
       "several     69\n",
       "first       68\n",
       "little      67\n",
       "good        58\n",
       "many        55\n",
       "much        52\n",
       "same        51\n",
       "old         48\n",
       "such        47\n",
       "black       44\n",
       "few         43\n",
       "young       42\n",
       "whole       32\n",
       "red         31\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(d).sort_values(ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm very surprised to see `i` at the top of this list; I'm not sure why `nltk` marked these instances as adjectives.\n",
    "\n",
    "However, many of the other words seem to be both accurately tagged and intellectually useful: It makes sense that the narrator's primary description of experiences would be as `white`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you could also use these methods to count *all* of the parts of speech in a given text, or across your entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-entity recognition\n",
    "We can use NLTK to identify named persons, places, and things in our texts through a process called named-entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "johnson = '/Users/e/Downloads/1912_johnson_ex-colored.txt'\n",
    "text = open(johnson).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Autobiography of an Ex-Colored Man\\n\\nJames Weldon Johnson\\n\\nBoston: Sherman, French & Company, 1912\\nCopyright, 1912\\n\\n\\n\\n\\nPREFACE\\n\\nThis vivid and startlingly new picture of conditions brought about by the race question in the United States makes no special plea for the Negro, but shows in a dispassionate, though sympathetic, manner conditions as they actually exist between the whites and blacks to-day.',\n",
       " 'Special pleas have already been made for and against the Negro in hundreds of books, but in these books either his virtues or his vices have been exaggerated.',\n",
       " 'This is because writers, in nearly every instance, have treated the colored American as a whole; each has taken some one group of the race to prove his case.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We start with raw sentences:\n",
    "sents = nltk.sent_tokenize(text)\n",
    "sents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a function for this\n",
    "def ner_sents(sent_list):\n",
    "    \"\"\"\n",
    "    Using NLTK, this function takes a list of sentences, identifies the named entities in it,\n",
    "    and returns a list of dictionaries, with one dictionary per named entitiy,\n",
    "    where each dictionary looks like this:\n",
    "    \n",
    "    {\n",
    "        'type': 'PERSON',\n",
    "        'entity': 'Harry Potter',\n",
    "        '_sent_num': 7,\n",
    "        '_sent': 'And why, Snape, is Harry Potter still alive, when you have had him at your mercy for five years?'\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # set empty list for output\n",
    "    output_list = []\n",
    "    \n",
    "    # loop over each sentence\n",
    "    for sent_num, sent in enumerate(sent_list):        \n",
    "        # we need to get the words\n",
    "        sent_words = nltk.word_tokenize(sent)\n",
    "        \n",
    "        # parts of speech\n",
    "        sent_pos = nltk.pos_tag(sent_words)\n",
    "        \n",
    "        # then \"chunk\" the results using ne_chunk\n",
    "        chunks = nltk.ne_chunk(sent_pos)\n",
    "        \n",
    "        # loop over chunks...\n",
    "        for chunk in chunks:\n",
    "            # if the chunk has a 'label' attribute (i.e. the entity is labeled)\n",
    "            if hasattr(chunk,'label'):\n",
    "                \n",
    "                # get the label\n",
    "                label = chunk.label()\n",
    "                \n",
    "                # get the words in the chunk\n",
    "                chunk_words = []\n",
    "                for word,pos in chunk:\n",
    "                    chunk_words.append(word)\n",
    "                \n",
    "                # make a string of the words\n",
    "                chunk_words_str = ' '.join(chunk_words)\n",
    "                \n",
    "                # make a result dictionary\n",
    "                result_dict = {}\n",
    "                \n",
    "                # add NER info\n",
    "                result_dict['type'] = label\n",
    "                result_dict['entity'] = chunk_words_str\n",
    "                \n",
    "                ### add sentence info\n",
    "                result_dict['sent_num'] = sent_num\n",
    "                # add a string of the sentence\n",
    "                result_dict['sent'] = sent\n",
    "                \n",
    "                # add result dictionary to output list\n",
    "                output_list.append(result_dict)\n",
    "    \n",
    "    # return list of dictionaries\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes about 60 seconds to run on my computer\n",
    "test = ner_sents(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-8a4b4c2a9a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clearly* `nltk` is imperfect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>sent</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James Weldon</td>\n",
       "      <td>The Autobiography of an Ex-Colored Man\\n\\nJame...</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnson Boston</td>\n",
       "      <td>The Autobiography of an Ex-Colored Man\\n\\nJame...</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sherman</td>\n",
       "      <td>The Autobiography of an Ex-Colored Man\\n\\nJame...</td>\n",
       "      <td>0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>French</td>\n",
       "      <td>The Autobiography of an Ex-Colored Man\\n\\nJame...</td>\n",
       "      <td>0</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Company</td>\n",
       "      <td>The Autobiography of an Ex-Colored Man\\n\\nJame...</td>\n",
       "      <td>0</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           entity                                               sent  \\\n",
       "0    James Weldon  The Autobiography of an Ex-Colored Man\\n\\nJame...   \n",
       "1  Johnson Boston  The Autobiography of an Ex-Colored Man\\n\\nJame...   \n",
       "2         Sherman  The Autobiography of an Ex-Colored Man\\n\\nJame...   \n",
       "3          French  The Autobiography of an Ex-Colored Man\\n\\nJame...   \n",
       "4         Company  The Autobiography of an Ex-Colored Man\\n\\nJame...   \n",
       "\n",
       "   sent_num          type  \n",
       "0         0        PERSON  \n",
       "1         0        PERSON  \n",
       "2         0        PERSON  \n",
       "3         0           GPE  \n",
       "4         0  ORGANIZATION  "
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that doesn't mean it's not useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "South                 45\n",
       "New York              39\n",
       "Paris                 33\n",
       "Negro                 30\n",
       "Southern              19\n",
       "United States         17\n",
       "French                17\n",
       "London                16\n",
       "Atlanta               15\n",
       "Jacksonville          14\n",
       "Washington            12\n",
       "Texan                 11\n",
       "Spanish               10\n",
       "Europe                10\n",
       "English               10\n",
       "American              10\n",
       "Negroes                9\n",
       "North                  8\n",
       "Shiny                  7\n",
       "Nashville              7\n",
       "Boston                 6\n",
       "Bible                  6\n",
       "Atlanta University     5\n",
       "America                5\n",
       "John Brown             5\n",
       "University             5\n",
       "Sixth Avenue           5\n",
       "Connecticut            5\n",
       "Union                  5\n",
       "American Negro         5\n",
       "Name: entity, dtype: int64"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entity'].value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPE             409\n",
       "PERSON          139\n",
       "ORGANIZATION     98\n",
       "LOCATION         23\n",
       "GSP              16\n",
       "FACILITY          7\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use NER?\n",
    "\n",
    "NER can be very useful if you want to look at features like geography, or frequently referenced presons. For example, we get some real specifics here: Sixth Avenue refers to the specific place in New York, near the top of the list. Note, too, that it is able to identify both single words and multiple words that comprise named entities in the world.\n",
    "\n",
    "We could use the resulting dataframe to look at the sentences where Sixth Avenue appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As soon as we landed, four of us went directly to a lodging-house in 27th Street, just west of Sixth Avenue.',\n",
       " 'We went to Sixth Avenue, walked two blocks, and turned to the west into another street.',\n",
       " 'We got out of the house about dark, went round to a restaurant on Sixth Avenue and ate something, then walked around for a couple of hours.',\n",
       " 'My New York was limited to ten blocks; the boundaries were Sixth Avenue from Twenty-third to Thirty-third Streets, with the cross streets one block to the west.',\n",
       " 'I went to Coney Island and the other resorts; took in the pre-season shows along Broadway, and ate at first class restaurants; but I shunned the old Sixth Avenue district as though it were pest infected.']"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df[df['entity'] == 'Sixth Avenue']['sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sixth Avenue comes to mean quite a lot to the protagonist after he begins passing for white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving part-of-speech tagged texts to disk\n",
    "Since POS tagging requires a good amount of computational power, you may want to save your reuslts if you intend to process the text multiple times.\n",
    "\n",
    "`nltk` has a standard way of storing POS-tagged data that we can write out to file to reimport later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `tuple2str` converts a `('word','pos')` tuple to a standard string representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in tagged_sents[49]:\n",
    "    print(nltk.tuple2str(tup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write each of these in sequence to a text file for later use using a new filemaking convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/e/Downloads/output.txt', mode = 'w') as output:\n",
    "    for sent in tagged_sents:\n",
    "        for tup in sent:        \n",
    "            output.write(nltk.tuple2str(tup) + ' ') # we are adding one space between every word/pos pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This begins with a file-handler expression, `with`. It opens the file, and the `as` statements assigns a variable name that we can use for that file during this operation.\n",
    "\n",
    "Just as we would `open()` an existing file on our hard drive and `read()` it into memory, we can also use `open()` to create a new file and add data to it. In order to do that, we have to change the `mode` we use to `open` the file. The default `mode` is `r`, which stands for \"read.\" That is what we have been using all quarter. `mode='w'` stands for \"write,\" and it's what we want to use when we wish to add data to a file that didn't exist before.\n",
    "\n",
    "**Warning**: Opening a file in mode `w` *deletes everything inside of it*. In the above case this is fine because there was nothing in my Downloads called `output.txt`. You only want to use mode `w` when you are creating new data, or overwriting old data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reimporting our tagged data\n",
    "Now we can use the reverse function, `str2tuple` to import our part-of-speech tagged data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/e/Downloads/output.txt', mode = 'r') as data: #note that mode has changed to r since we are reading\n",
    "    data = data.read().split(' ')\n",
    "    tagged_text = []\n",
    "    for tup in data:\n",
    "        tagged_text.append(nltk.str2tuple(tup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are processes that take your computer a long time to execute, it is often good practice to write your results to disk somewhere so that you can access them later without having to re-process everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating part-of-speech data\n",
    "Finally, let's say that we want to count up instances of words used as specific parts of speech.\n",
    "\n",
    "One approach we could use would be to use our usual counting techniques *directly on the POS output* we generated above.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open('/Users/e/Downloads/output.txt') as data:\n",
    "    words = data.read().split(' ')\n",
    "    result = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could easily apply the same technique to an entire corpus by performing the following steps:\n",
    "1. POS tagging each file\n",
    "2. Writing the POS-tagged versions of each file to disk\n",
    "3. Reading in each POS-tagged file\n",
    "4. Counting the POS pairs as above\n",
    "5. Adding filename information to the dictionary like so:\n",
    "```python\n",
    "d['filename'] = 'output.txt'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
