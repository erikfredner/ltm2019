{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading your corpus\n",
    "It's important to know what's **in** your corpus before you begin working with it. One of the best ways to do that is to look through the files manually and confirm that they are what we expect them to be.\n",
    "\n",
    "### a. Look over at least 5 text files in your group's corpus. Scroll around and read a few lines of each.\n",
    "\n",
    "How do they look? Do they have any obvious typos or errors? Are there unusual characters or clearly misspelled words? Are they long enough? Are there formatting problems that need to be cleaned up? Anything else you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rig up your corpus\n",
    "Now that you've read through your corpus, it's time to set it up for analysis.\n",
    "\n",
    "*The notebooks for 10-28 and 10-30 contain everything you need to get through this problem very quickly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Make a document-term matrix of your corpus with raw frequencies. Be sure to set your filename (or another unique value) as your index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_raw = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Make a document-term matrix of your corpus with frequencies scaled by the total number of words in each text. Be sure to set your filename (or another unique value) as your index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_scaled = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Explain the difference between document-term matrices with raw and scaled frequencies. What is the value of scaling? When might you want to use raw frequencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Practicing with Pandas\n",
    "Now that we have loaded your corpus as a data frame, we're going to practice manipulating it with Pandas. If you're lucky, you'll start to see some interesting results for your final project!\n",
    "\n",
    "Again, check the notebooks for 10-28 and 10-30 for examples of each of the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Pick a word you're interested in from your DTM's columns. Print out its scaled frequencies in each of your texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Now, sort those values such that the highest values are at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Pick a second word you want to compare to your first word. Print a dataframe containing *only* those two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. What are the most frequent words in your corpus? Sum all of your columns, and sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. How many words in your raw DTM only appear 1 time? Sum your columns, and filter for rows `==1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding metadata to your scaled DTM\n",
    "\n",
    "Now we're going to practice adding metadata to your data frame. This will allow you to easily sort, filter, and group your results.\n",
    "\n",
    "We talked about this in class on Monday, but here's a refresher on how to `merge` data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say we're working with data from a cafe:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "coffee = {'food':'coffee','size':'8oz','price':'$3.00'}\n",
    "banana ={'food':'banana','size':'4oz','price':'$0.50'}\n",
    "donut = {'food':'donut', 'size':'6oz', 'price':'$1.00'}\n",
    "food_df = pd.DataFrame([coffee,banana,donut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>$3.00</td>\n",
       "      <td>8oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>4oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>donut</td>\n",
       "      <td>$1.00</td>\n",
       "      <td>6oz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     food  price size\n",
       "0  coffee  $3.00  8oz\n",
       "1  banana  $0.50  4oz\n",
       "2   donut  $1.00  6oz"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to combine that data with some other data about when and how those foods sell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_sales = {'food':'coffee', 'peak sales':'9am', 'total sold':181}\n",
    "banana_sales = {'food':'banana', 'peak sales':'1pm', 'total sold':36}\n",
    "donut_sales = {'food':'donut', 'peak sales':'10am', 'total sold':96}\n",
    "sales_df = pd.DataFrame([coffee_sales, banana_sales, donut_sales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>peak sales</th>\n",
       "      <th>total sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>9am</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>1pm</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>donut</td>\n",
       "      <td>10am</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     food peak sales  total sold\n",
       "0  coffee        9am         181\n",
       "1  banana        1pm          36\n",
       "2   donut       10am          96"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine these data frames using `pd.merge`. In order for it to work, you have to pass **a column that both data frames share in common** to the `on` argument (\"on\" as in \"merge the dataframes *on* this column\").\n",
    "\n",
    "In this case, both data frames have the column `food` in common. So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>peak sales</th>\n",
       "      <th>total sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coffee</td>\n",
       "      <td>$3.00</td>\n",
       "      <td>8oz</td>\n",
       "      <td>9am</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>4oz</td>\n",
       "      <td>1pm</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>donut</td>\n",
       "      <td>$1.00</td>\n",
       "      <td>6oz</td>\n",
       "      <td>10am</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     food  price size peak sales  total sold\n",
       "0  coffee  $3.00  8oz        9am         181\n",
       "1  banana  $0.50  4oz        1pm          36\n",
       "2   donut  $1.00  6oz       10am          96"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(food_df, sales_df, on='food')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Use `pd.merge` to add the `PUBL_DATE` column from your metadata file to your DTM.\n",
    "1. Import your metadata using `pd.read_csv`\n",
    "2. Slice your metadata so that you only have the `PUBL_DATE` column and the column you want to match on.\n",
    "3. Merge your DTM with your metadata slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Sort your DTM by year using the `PUBL_DATE` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Getting results with `corp_collocates`\n",
    "The functions for `corp_collocates` are below. Be sure to run them all in order for the functions to work.\n",
    "\n",
    "We're going use them to start looking for results in your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function depends upon a few of our old friends like absolute_paths and tokenizer\n",
    "# txt_dir points to a directory where your text files are located, and stored in .txt format\n",
    "\n",
    "def corp_collocates(word, txt_dir, horizon = 10, percentile = 0.9, drop_stopwords = True):\n",
    "    # 1. generate a list of files\n",
    "    filepaths = absolute_paths(txt_dir)\n",
    "    \n",
    "    # 2. make a list of dictionaries containing our data\n",
    "    output = []\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        collocates = get_collocates(filepath, word, horizon)\n",
    "        output.append(collocates)\n",
    "    \n",
    "    # 3. make a dataframe of our results\n",
    "    dtm = pd.DataFrame(output)\n",
    "    dtm = dtm.set_index(['filepath', 'target_word']).sort_index()\n",
    "    \n",
    "    # 4. optionally drop stopwords\n",
    "    keep = []\n",
    "    if drop_stopwords is True:\n",
    "        for x in dtm.columns:\n",
    "            if x not in stopwords:    \n",
    "                keep.append(x)\n",
    "    \n",
    "        dtm = dtm[keep]        \n",
    "        \n",
    "    # 5. sum dtm and cut to percentile\n",
    "    sums = dtm.sum()\n",
    "    pct_index = round(len(sums) * percentile)\n",
    "    top_words = sums.sort_values()[pct_index:].index # index returns the list of words\n",
    "    \n",
    "    # 6. scale results\n",
    "    dtm = dtm[top_words]\n",
    "    raw_values = make_dtm(txt_dir)[top_words]\n",
    "    scaled_results = dtm.sum() / raw_values.sum()\n",
    "    \n",
    "    return scaled_results.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_dtm(directory, scaled = False):\n",
    "    files = absolute_paths(directory)\n",
    "    \n",
    "    result = [] # empty list where I will append the dictionaries of word counts\n",
    "    \n",
    "    for file in files: # looping over the results\n",
    "        text = open(file).read() # read in text file\n",
    "        tokens = tokenize(text) # make tokens list\n",
    "        d = count_words(tokens) # use count_words to create a dictionary\n",
    "        \n",
    "        if scaled is True:\n",
    "            total_words = sum(list(d.values()))\n",
    "            for key,value in d.items():\n",
    "                d[key] = d[key] / total_words\n",
    "        \n",
    "        # os.path.split() returns the base path and the filename as a pair:\n",
    "        d['filepath'] = os.path.split(file)[-1] # include the _ before filename in case the text contains \"filename\"\n",
    "        result.append(d) # append the unscaled result\n",
    "    \n",
    "    return pd.DataFrame(result).set_index('filepath').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collocates(filepath, target_word, horizon = 10):\n",
    "    text = open(filepath).read() # get text\n",
    "    tokens = tokenize(text) # get tokens\n",
    "    \n",
    "    indexes = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == target_word:\n",
    "            indexes.append(i) # get indexes\n",
    "    \n",
    "    collocates = []\n",
    "\n",
    "    for index in indexes:\n",
    "        colls = tokens[index-10:index+10]\n",
    "        del colls[round(len(colls)/2)] # don't count target term\n",
    "        collocates.extend(colls) # we use extend rather than append because we are adding additional elements *from* a list\n",
    "        \n",
    "    d = {}\n",
    "    # we want to make sure we get data about where our values are coming from. this tells us the file:\n",
    "    d['filepath'] = os.path.split(filepath)[-1] \n",
    "    d['target_word'] = target_word # this tells us our target\n",
    "    \n",
    "    for coll in collocates:\n",
    "        if coll not in d:\n",
    "            d[coll] = 1 # count up collocates\n",
    "        else:\n",
    "            d[coll] += 1\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def tokenize(text, keep_punct = False):\n",
    "    if keep_punct is True:\n",
    "        for punct in string.punctuation:\n",
    "            text = text.replace(punct, ' ' + punct + ' ')\n",
    "    else:\n",
    "        for punct in string.punctuation:\n",
    "            text = text.replace(punct, ' ')\n",
    "    \n",
    "    # this replaces *any* amount of whitespace with a single space using regular expressions\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for x in text.lower().split(' '):\n",
    "        if x.isalpha():\n",
    "            result.append(x)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def absolute_paths(directory, txt_only = True):\n",
    "    files = os.listdir(directory)\n",
    "    absolute_paths = []\n",
    "    \n",
    "    for file in files:\n",
    "        path = os.path.join(directory, file)\n",
    "        absolute_paths.append(path)\n",
    "    \n",
    "    if txt_only is True:\n",
    "        txts = []\n",
    "        for x in absolute_paths:\n",
    "            if str('.txt') in str(x):\n",
    "                txts.append(x)\n",
    "        return txts\n",
    "    \n",
    "    else:        \n",
    "        return absolute_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(word_list):\n",
    "    d = {}\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word not in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " \"you'll\",\n",
    " \"you'd\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'ma',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shaggy        0.555556\n",
       "gamekeeper    0.545455\n",
       "gotta         0.500000\n",
       "steak         0.444444\n",
       "yeh           0.412587\n",
       "bin           0.373333\n",
       "fer           0.370787\n",
       "gruffly       0.370370\n",
       "committee     0.347826\n",
       "fence         0.346154\n",
       "yer           0.340659\n",
       "ter           0.338192\n",
       "umbrella      0.333333\n",
       "skrewts       0.325000\n",
       "thestrals     0.310345\n",
       "fang          0.271739\n",
       "cabin         0.264151\n",
       "aragog        0.256410\n",
       "maxime        0.244898\n",
       "massive       0.240000\n",
       "sadly         0.219512\n",
       "creatures     0.217949\n",
       "growled       0.211111\n",
       "wiping        0.204082\n",
       "beard         0.195402\n",
       "ended         0.194444\n",
       "dragons       0.193548\n",
       "madame        0.192308\n",
       "buckbeak      0.188119\n",
       "grawp         0.185185\n",
       "                ...   \n",
       "nothing       0.020031\n",
       "black         0.020022\n",
       "day           0.019651\n",
       "hall          0.019576\n",
       "bit           0.019194\n",
       "robes         0.019149\n",
       "something     0.018949\n",
       "though        0.018847\n",
       "next          0.018786\n",
       "year          0.018762\n",
       "felt          0.018391\n",
       "going         0.018182\n",
       "moment        0.018141\n",
       "many          0.017279\n",
       "air           0.017143\n",
       "mind          0.016878\n",
       "every         0.016287\n",
       "find          0.015748\n",
       "upon          0.015576\n",
       "boy           0.014925\n",
       "mr            0.014178\n",
       "sure          0.013850\n",
       "floor         0.013699\n",
       "lupin         0.013624\n",
       "dark          0.013566\n",
       "snape         0.010556\n",
       "weasley       0.010381\n",
       "sirius        0.010331\n",
       "room          0.008955\n",
       "wand          0.007212\n",
       "Length: 361, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_collocates('hagrid', '/Users/e/code/literarytextmining/corpora/harry_potter/texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Find at least 3 collocations of interest for your project, and write a sentence about the potential significance of each. (Be sure to print the collocates you discuss above so I can see them!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your response here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Then, write a sentence reflecting on the value of collocation as a method of analysis. What does it tell you? What more would you want to know about your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your response here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
